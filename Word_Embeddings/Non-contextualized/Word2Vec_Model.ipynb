{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad20c482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "seed = 1\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9bec4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "import gensim.downloader as api\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adbdb43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.layers import Dense, Embedding, Conv1D, GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c7a1d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9eab47ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_sequence(questions):\n",
    "    X = questions['Preprocessed_Question']\n",
    "    cognitive_level = {\"Knowledge\": 0, \"Comprehension\": 1, \"Application\": 2, \"Analysis\": 3, \"Synthesis\": 4, \"Evaluation\": 5}\n",
    "    questions[\"BT LEVEL\"].replace(cognitive_level, inplace = True)\n",
    "    y = questions['BT LEVEL'].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y , test_size = 0.10, \n",
    "                                        stratify = questions['BT LEVEL'], random_state = 1)\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    \n",
    "    tokenizer = Tokenizer(oov_token ='<OOV>')\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    vocab = tokenizer.word_index\n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "    X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "    max_que_len = max(len(x) for x in X_train_seq)\n",
    "    padded_X_train_seq = pad_sequences(X_train_seq, maxlen = max_que_len, padding = 'post')\n",
    "    padded_X_test_seq = pad_sequences(X_test_seq, maxlen = max_que_len, padding = 'post')\n",
    "    \n",
    "    return padded_X_train_seq, y_train, padded_X_test_seq, y_test, vocab, max_que_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8693fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(vocab):\n",
    "    embedding_model = api.load('word2vec-google-news-300')\n",
    "    embeddings = np.zeros((len(vocab) + 1, 300))\n",
    "    for word, index in vocab.items():\n",
    "        if word in embedding_model.key_to_index:\n",
    "            embedding_vector = embedding_model[word]\n",
    "            embeddings[index] = embedding_vector\n",
    "        else:\n",
    "            print(word)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a30c351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_len, max_que_len, embeddings):\n",
    "  \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim = vocab_size, output_dim = embedding_len, weights = [embeddings], \n",
    "                        input_length = max_que_len,  trainable = True))\n",
    "    model.add(Conv1D(32, 5, activation ='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(6, activation ='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db4008fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    pred = model.predict(X_test)\n",
    "    predicted_classes = np.argmax(pred, axis = 1)\n",
    "    true_classes = np.argmax(y_test, axis = 1)\n",
    "    accuracy = accuracy_score(true_classes, predicted_classes)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    f1_ = f1_score(true_classes, predicted_classes, average = 'weighted')\n",
    "    print(f\"F1 score: {f1_}\")\n",
    "    print(classification_report(true_classes, predicted_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc3ecc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(log_data): \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(log_data['loss'], label ='train')\n",
    "    plt.plot(log_data['val_loss'], label ='test')\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(log_data['accuracy'], label ='train')\n",
    "    plt.plot(log_data['val_accuracy'], label ='test')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(log_data['f1_score'], label ='train')\n",
    "    plt.plot(log_data['val_f1_score'], label ='test')\n",
    "    plt.title('Model F1 Score')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3af42cbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Preprocessed_Question</th>\n",
       "      <th>BT LEVEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Suppose prices of two goods are constant, expl...</td>\n",
       "      <td>suppose price good constant explain what happe...</td>\n",
       "      <td>Comprehension</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Explain the concept of price leadership observ...</td>\n",
       "      <td>explain concept price leadership observe condi...</td>\n",
       "      <td>Comprehension</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Define profit. Briefly explain how accounting ...</td>\n",
       "      <td>define profit briefly explain how account prof...</td>\n",
       "      <td>Comprehension</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Describe the assumptions of monopolistic compe...</td>\n",
       "      <td>describe assumption monopolistic competitive m...</td>\n",
       "      <td>Comprehension</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Explain the meaning of the law of diminishing ...</td>\n",
       "      <td>explain mean law diminish marginal return brie...</td>\n",
       "      <td>Comprehension</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2517</th>\n",
       "      <td>PEST and SWOT are popular strategy tools. Disc...</td>\n",
       "      <td>pest swot popular strategy tool discuss how to...</td>\n",
       "      <td>Application</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2518</th>\n",
       "      <td>List the advantages and disadvantages of Publi...</td>\n",
       "      <td>list advantage disadvantage public offer</td>\n",
       "      <td>Knowledge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2519</th>\n",
       "      <td>Show your calculations for all THREE (3) optio...</td>\n",
       "      <td>show calculation option discuss which option p...</td>\n",
       "      <td>Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2520</th>\n",
       "      <td>Currently the product life cycle for Apple iPo...</td>\n",
       "      <td>currently product life cycle apple ipod growth...</td>\n",
       "      <td>Evaluation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2521</th>\n",
       "      <td>Define brand audit.</td>\n",
       "      <td>define brand audit</td>\n",
       "      <td>Knowledge</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2522 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Question  \\\n",
       "0     Suppose prices of two goods are constant, expl...   \n",
       "1     Explain the concept of price leadership observ...   \n",
       "2     Define profit. Briefly explain how accounting ...   \n",
       "3     Describe the assumptions of monopolistic compe...   \n",
       "4     Explain the meaning of the law of diminishing ...   \n",
       "...                                                 ...   \n",
       "2517  PEST and SWOT are popular strategy tools. Disc...   \n",
       "2518  List the advantages and disadvantages of Publi...   \n",
       "2519  Show your calculations for all THREE (3) optio...   \n",
       "2520  Currently the product life cycle for Apple iPo...   \n",
       "2521                                Define brand audit.   \n",
       "\n",
       "                                  Preprocessed_Question       BT LEVEL  \n",
       "0     suppose price good constant explain what happe...  Comprehension  \n",
       "1     explain concept price leadership observe condi...  Comprehension  \n",
       "2     define profit briefly explain how account prof...  Comprehension  \n",
       "3     describe assumption monopolistic competitive m...  Comprehension  \n",
       "4     explain mean law diminish marginal return brie...  Comprehension  \n",
       "...                                                 ...            ...  \n",
       "2517  pest swot popular strategy tool discuss how to...    Application  \n",
       "2518           list advantage disadvantage public offer      Knowledge  \n",
       "2519  show calculation option discuss which option p...       Analysis  \n",
       "2520  currently product life cycle apple ipod growth...     Evaluation  \n",
       "2521                                 define brand audit      Knowledge  \n",
       "\n",
       "[2522 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = pd.read_excel('preprocessing_result/preprocessing_result-w2v.xlsx')\n",
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "260da8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_X_train_seq, y_train, padded_X_test_seq, y_test, vocab, max_que_len = convert_to_sequence(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00df7e54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<OOV>\n",
      "tqm\n",
      "aov\n",
      "bjt\n",
      "nyquist\n",
      "edman\n",
      "sdn\n",
      "bhd\n",
      "infomediary\n",
      "bnm\n",
      "and\n",
      "phosphoru\n",
      "treynor\n",
      "venn\n",
      "mccg\n",
      "glutamic\n",
      "of\n",
      "maslow\n",
      "langmuir\n",
      "blodgett\n",
      "komugi\n",
      "nlp\n",
      "eoq\n",
      "mlp\n",
      "finfet\n",
      "pcr\n",
      "to\n",
      "hofstede\n",
      "mvc\n",
      "mesophyll\n",
      "steagall\n",
      "a\n",
      "berhad\n",
      "utar\n",
      "chemostat\n",
      "schein\n",
      "cambodia\n",
      "radiobutton\n",
      "mrna\n",
      "ldh\n",
      "fermi\n",
      "markov\n",
      "mooc\n",
      "perak\n",
      "adts\n",
      "kaseem\n",
      "airasia\n",
      "perakian\n",
      "cvn\n",
      "junebank\n",
      "rna\n",
      "firedup\n",
      "qlassic\n",
      "gcb\n",
      "irr\n",
      "npv\n",
      "mirr\n",
      "aspartic\n",
      "genbank\n",
      "mrp\n",
      "pondy\n",
      "quasineutral\n",
      "addie\n",
      "ehp\n",
      "alavi\n",
      "leidner\n",
      "cec\n",
      "abap\n",
      "openerp\n",
      "sugarcrm\n",
      "columbiana\n",
      "spiration\n",
      "cooperativity\n",
      "electroosmosis\n",
      "greimas\n",
      "vlsi\n",
      "hexapeptide\n",
      "kmk\n",
      "nextseq\n",
      "schrodinger\n",
      "chymotrypsin\n",
      "passivator\n",
      "dfa\n",
      "arraylist\n",
      "bubblesort\n",
      "quicksort\n",
      "matrik\n",
      "trna\n",
      "rct\n",
      "putlog\n",
      "cadbury\n",
      "dirac\n",
      "welliver\n",
      "kampar\n",
      "radley\n",
      "blosum\n",
      "vle\n",
      "rle\n",
      "jusco\n",
      "kinta\n",
      "ipoh\n",
      "pseudocode\n",
      "kahneman\n",
      "tversky\n",
      "dictogloss\n",
      "hyperarid\n",
      "nonaka\n",
      "eqrnhl\n",
      "loreal\n",
      "spss\n",
      "neolocal\n",
      "gaucher\n",
      "michaeli\n",
      "faraday\n",
      "sawaya\n",
      "browne\n",
      "laplace\n",
      "sethour\n",
      "refseq\n",
      "solum\n",
      "sendredirect\n",
      "linebreeding\n",
      "huffman\n",
      "phenylpropanoid\n",
      "pourbaix\n",
      "howie\n",
      "mesocarp\n",
      "minisatellite\n",
      "realaudio\n",
      "oxyperoxidase\n",
      "luedeking\n",
      "piret\n",
      "pullulan\n",
      "cladogram\n",
      "eurocurrency\n",
      "mesofauna\n",
      "hersey\n",
      "blanchard\n",
      "tdap\n",
      "cominunicate\n",
      "medoids\n",
      "diskretionary\n",
      "scop\n",
      "weinberg\n",
      "langmiur\n",
      "isoterm\n",
      "laccase\n",
      "opac\n",
      "requestdispatcher\n",
      "mircroscope\n",
      "afm\n",
      "autosome\n",
      "myer\n",
      "higg\n",
      "greenbury\n",
      "weick\n",
      "httpsessionactivationlistener\n",
      "mincut\n",
      "determinental\n",
      "uam\n",
      "tomlinson\n",
      "marginalism\n",
      "engel\n",
      "schubli\n",
      "pareto\n",
      "kolmogorov\n",
      "mediuin\n",
      "sqykrecp\n",
      "myner\n",
      "programrs\n",
      "wmeadk\n",
      "dmvrel\n",
      "mmlr\n",
      "allelopathy\n",
      "luzonensis\n",
      "ffr\n",
      "ionizable\n",
      "prenyltransferase\n",
      "exarnple\n",
      "blastx\n",
      "matrilocal\n",
      "sqa\n",
      "pnk\n",
      "equillibrium\n",
      "setia\n",
      "avc\n",
      "atc\n",
      "macaulay\n",
      "suriname\n",
      "musictype\n",
      "mytune\n",
      "fupo\n",
      "satisfiability\n",
      "sld\n",
      "pointwise\n",
      "placentome\n",
      "mcdonough\n",
      "matsuhara\n",
      "nacl\n",
      "deae\n",
      "sephadex\n",
      "phosphatidylethanolamine\n",
      "jstl\n",
      "pmb\n",
      "hexokinase\n",
      "krw\n",
      "elaeis\n",
      "guineensis\n",
      "monoecious\n",
      "eisenthal\n",
      "bowden\n",
      "gly\n",
      "glu\n",
      "modelessform\n",
      "showdialog\n",
      "karush\n",
      "herbatious\n",
      "aras\n",
      "millar\n",
      "jfet\n",
      "makamat\n",
      "incfromn\n",
      "sooosay\n",
      "gml\n",
      "hato\n",
      "giffen\n",
      "emh\n",
      "dpm\n",
      "instagram\n",
      "outli\n",
      "ytm\n",
      "zymogen\n",
      "mouton\n",
      "komive\n",
      "bolitho\n",
      "ncbi\n",
      "aco\n",
      "iliad\n",
      "valles\n",
      "marineris\n",
      "leggo\n",
      "olap\n",
      "tblastx\n",
      "viestal\n",
      "atcase\n",
      "atticus\n",
      "communalitie\n",
      "schottky\n",
      "miscroscope\n"
     ]
    }
   ],
   "source": [
    "embeddings_w2v = get_embedding(vocab)\n",
    "np.save('embedding/word2vec/embeddings.npy', embeddings_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e27e23c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_w2v = np.load('embedding/word2vec/embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b48c3a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#callbacks \n",
    "training_logger = CSVLogger('log/W2V/training.log', separator = ',', append = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cd1ba60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 44, 300)           1109700   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 40, 32)            48032     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 32)               0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 6)                 198       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,157,930\n",
      "Trainable params: 1,157,930\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model\n",
    "model = build_model(len(vocab) + 1, 300, max_que_len, embeddings_w2v)\n",
    "# compiling the model\n",
    "model.compile(optimizer = 'RMSprop', loss='categorical_crossentropy', metrics=['accuracy', tfa.metrics.F1Score(6, 'weighted')])\n",
    "# printing summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e36628d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "284/284 [==============================] - 5s 11ms/step - loss: 1.2551 - accuracy: 0.5619 - f1_score: 0.5257 - val_loss: 0.8010 - val_accuracy: 0.7747 - val_f1_score: 0.7710\n",
      "Epoch 2/50\n",
      "284/284 [==============================] - 3s 9ms/step - loss: 0.5430 - accuracy: 0.8541 - f1_score: 0.8530 - val_loss: 0.5932 - val_accuracy: 0.7905 - val_f1_score: 0.7906\n",
      "Epoch 3/50\n",
      "284/284 [==============================] - 3s 9ms/step - loss: 0.2573 - accuracy: 0.9295 - f1_score: 0.9293 - val_loss: 0.5243 - val_accuracy: 0.8221 - val_f1_score: 0.8198\n",
      "Epoch 4/50\n",
      "284/284 [==============================] - 3s 9ms/step - loss: 0.1156 - accuracy: 0.9766 - f1_score: 0.9766 - val_loss: 0.5259 - val_accuracy: 0.8024 - val_f1_score: 0.8013\n",
      "Epoch 5/50\n",
      "284/284 [==============================] - 3s 9ms/step - loss: 0.0508 - accuracy: 0.9881 - f1_score: 0.9881 - val_loss: 0.5729 - val_accuracy: 0.8142 - val_f1_score: 0.8131\n",
      "Epoch 6/50\n",
      "284/284 [==============================] - 3s 9ms/step - loss: 0.0208 - accuracy: 0.9938 - f1_score: 0.9938 - val_loss: 0.6032 - val_accuracy: 0.8142 - val_f1_score: 0.8119\n",
      "Epoch 7/50\n",
      "284/284 [==============================] - 3s 11ms/step - loss: 0.0095 - accuracy: 0.9982 - f1_score: 0.9982 - val_loss: 0.6893 - val_accuracy: 0.8142 - val_f1_score: 0.8116\n",
      "Epoch 8/50\n",
      "284/284 [==============================] - 3s 9ms/step - loss: 0.0038 - accuracy: 0.9996 - f1_score: 0.9996 - val_loss: 0.6920 - val_accuracy: 0.8261 - val_f1_score: 0.8242\n",
      "Epoch 9/50\n",
      "284/284 [==============================] - 3s 9ms/step - loss: 0.0020 - accuracy: 0.9996 - f1_score: 0.9996 - val_loss: 0.8032 - val_accuracy: 0.8142 - val_f1_score: 0.8124\n",
      "Epoch 10/50\n",
      "284/284 [==============================] - 2s 9ms/step - loss: 5.6629e-04 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.8970 - val_accuracy: 0.8142 - val_f1_score: 0.8115\n",
      "Epoch 11/50\n",
      "284/284 [==============================] - 2s 9ms/step - loss: 3.0470e-04 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.9450 - val_accuracy: 0.8261 - val_f1_score: 0.8253\n",
      "Epoch 12/50\n",
      "284/284 [==============================] - 3s 9ms/step - loss: 1.5817e-04 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.0352 - val_accuracy: 0.8024 - val_f1_score: 0.7988\n",
      "Epoch 13/50\n",
      "284/284 [==============================] - 3s 9ms/step - loss: 5.6611e-05 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.0714 - val_accuracy: 0.8103 - val_f1_score: 0.8077\n",
      "Epoch 14/50\n",
      "284/284 [==============================] - 3s 9ms/step - loss: 1.7906e-05 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.0956 - val_accuracy: 0.8103 - val_f1_score: 0.8087\n",
      "Epoch 15/50\n",
      "284/284 [==============================] - 3s 9ms/step - loss: 8.2417e-06 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.1563 - val_accuracy: 0.8103 - val_f1_score: 0.8082\n",
      "Epoch 16/50\n",
      "284/284 [==============================] - 3s 9ms/step - loss: 5.8412e-06 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.2113 - val_accuracy: 0.8103 - val_f1_score: 0.8077\n",
      "Epoch 17/50\n",
      "284/284 [==============================] - 2s 9ms/step - loss: 5.0331e-07 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.2536 - val_accuracy: 0.8063 - val_f1_score: 0.8035\n",
      "Epoch 18/50\n",
      "284/284 [==============================] - 3s 9ms/step - loss: 1.6171e-07 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.2526 - val_accuracy: 0.8063 - val_f1_score: 0.8035\n",
      "Epoch 19/50\n",
      "284/284 [==============================] - 3s 9ms/step - loss: 6.5988e-08 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.3120 - val_accuracy: 0.8063 - val_f1_score: 0.8035\n",
      "Epoch 20/50\n",
      "284/284 [==============================] - 3s 9ms/step - loss: 2.2644e-08 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.3099 - val_accuracy: 0.8024 - val_f1_score: 0.8002\n",
      "Epoch 21/50\n",
      "284/284 [==============================] - 3s 9ms/step - loss: 1.2557e-08 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.3378 - val_accuracy: 0.8024 - val_f1_score: 0.8003\n",
      "Epoch 22/50\n",
      "284/284 [==============================] - 3s 9ms/step - loss: 8.9840e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.3421 - val_accuracy: 0.8024 - val_f1_score: 0.8000\n",
      "Epoch 23/50\n",
      "284/284 [==============================] - 3s 9ms/step - loss: 6.6724e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.3537 - val_accuracy: 0.8024 - val_f1_score: 0.8000\n",
      "Epoch 24/50\n",
      "284/284 [==============================] - 2s 9ms/step - loss: 5.5691e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.3661 - val_accuracy: 0.8024 - val_f1_score: 0.8000\n",
      "Epoch 25/50\n",
      "284/284 [==============================] - 3s 9ms/step - loss: 4.4132e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.3775 - val_accuracy: 0.8024 - val_f1_score: 0.8000\n",
      "Epoch 26/50\n",
      "284/284 [==============================] - 3s 9ms/step - loss: 4.3607e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.3813 - val_accuracy: 0.8024 - val_f1_score: 0.8000\n",
      "Epoch 27/50\n",
      "284/284 [==============================] - 3s 9ms/step - loss: 4.2556e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.3917 - val_accuracy: 0.8024 - val_f1_score: 0.8000\n",
      "Epoch 28/50\n",
      "284/284 [==============================] - 3s 9ms/step - loss: 3.8878e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.3979 - val_accuracy: 0.7984 - val_f1_score: 0.7964\n",
      "Epoch 29/50\n",
      "284/284 [==============================] - 3s 9ms/step - loss: 3.8353e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.4005 - val_accuracy: 0.7984 - val_f1_score: 0.7964\n",
      "Epoch 30/50\n",
      "284/284 [==============================] - 3s 9ms/step - loss: 3.8353e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.4164 - val_accuracy: 0.7984 - val_f1_score: 0.7964\n",
      "Epoch 31/50\n",
      "284/284 [==============================] - 3s 9ms/step - loss: 4.0980e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.4236 - val_accuracy: 0.7984 - val_f1_score: 0.7964\n",
      "Epoch 32/50\n",
      "284/284 [==============================] - 2s 9ms/step - loss: 4.3081e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.4375 - val_accuracy: 0.7984 - val_f1_score: 0.7964\n",
      "Epoch 33/50\n",
      "284/284 [==============================] - 2s 9ms/step - loss: 4.2031e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.4430 - val_accuracy: 0.7945 - val_f1_score: 0.7925\n",
      "Epoch 34/50\n",
      "284/284 [==============================] - 3s 9ms/step - loss: 3.9929e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.4420 - val_accuracy: 0.7945 - val_f1_score: 0.7927\n",
      "Epoch 35/50\n",
      "284/284 [==============================] - 3s 9ms/step - loss: 4.3081e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.4439 - val_accuracy: 0.7945 - val_f1_score: 0.7922\n",
      "Epoch 36/50\n",
      "284/284 [==============================] - 3s 9ms/step - loss: 4.7284e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.4610 - val_accuracy: 0.7945 - val_f1_score: 0.7920\n",
      "Epoch 37/50\n",
      "284/284 [==============================] - 2s 9ms/step - loss: 3.7302e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.4706 - val_accuracy: 0.7984 - val_f1_score: 0.7958\n",
      "Epoch 38/50\n",
      "284/284 [==============================] - 3s 9ms/step - loss: 4.7284e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.4731 - val_accuracy: 0.7984 - val_f1_score: 0.7963\n",
      "Epoch 39/50\n",
      "284/284 [==============================] - 2s 9ms/step - loss: 4.3081e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.4741 - val_accuracy: 0.7984 - val_f1_score: 0.7963\n",
      "Epoch 40/50\n",
      "284/284 [==============================] - 3s 9ms/step - loss: 4.8335e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.4900 - val_accuracy: 0.8024 - val_f1_score: 0.7999\n",
      "Epoch 41/50\n",
      "284/284 [==============================] - 3s 9ms/step - loss: 4.5708e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.4849 - val_accuracy: 0.7984 - val_f1_score: 0.7959\n",
      "Epoch 42/50\n",
      "284/284 [==============================] - 3s 9ms/step - loss: 5.2013e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.5004 - val_accuracy: 0.8024 - val_f1_score: 0.7999\n",
      "Epoch 43/50\n",
      "284/284 [==============================] - 3s 9ms/step - loss: 5.0437e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.5225 - val_accuracy: 0.8024 - val_f1_score: 0.8000\n",
      "Epoch 44/50\n",
      "284/284 [==============================] - 2s 9ms/step - loss: 5.3589e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.5266 - val_accuracy: 0.7984 - val_f1_score: 0.7959\n",
      "Epoch 45/50\n",
      "284/284 [==============================] - 2s 9ms/step - loss: 4.9911e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.5190 - val_accuracy: 0.8024 - val_f1_score: 0.7995\n",
      "Epoch 46/50\n",
      "284/284 [==============================] - 2s 9ms/step - loss: 5.6216e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.5368 - val_accuracy: 0.7984 - val_f1_score: 0.7959\n",
      "Epoch 47/50\n",
      "284/284 [==============================] - 2s 9ms/step - loss: 5.7792e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.5394 - val_accuracy: 0.7984 - val_f1_score: 0.7959\n",
      "Epoch 48/50\n",
      "284/284 [==============================] - 3s 9ms/step - loss: 5.5165e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.5568 - val_accuracy: 0.8024 - val_f1_score: 0.7995\n",
      "Epoch 49/50\n",
      "284/284 [==============================] - 2s 9ms/step - loss: 6.1995e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.5564 - val_accuracy: 0.8024 - val_f1_score: 0.7995\n",
      "Epoch 50/50\n",
      "284/284 [==============================] - 3s 9ms/step - loss: 5.8843e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.5734 - val_accuracy: 0.8024 - val_f1_score: 0.7995\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(padded_X_train_seq, y_train, epochs = 50, batch_size = 8, validation_data = (padded_X_test_seq, y_test),\n",
    "                                                                      callbacks = [training_logger], verbose = 1)\n",
    "#saving the trained model\n",
    "model.save('saved_models/W2V/EQCM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd9d157f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_f1_score</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.561922</td>\n",
       "      <td>0.525717</td>\n",
       "      <td>1.255105e+00</td>\n",
       "      <td>0.774704</td>\n",
       "      <td>0.770956</td>\n",
       "      <td>0.800958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.854121</td>\n",
       "      <td>0.853032</td>\n",
       "      <td>5.430162e-01</td>\n",
       "      <td>0.790514</td>\n",
       "      <td>0.790615</td>\n",
       "      <td>0.593165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.929484</td>\n",
       "      <td>0.929341</td>\n",
       "      <td>2.573461e-01</td>\n",
       "      <td>0.822134</td>\n",
       "      <td>0.819784</td>\n",
       "      <td>0.524282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.976642</td>\n",
       "      <td>0.976646</td>\n",
       "      <td>1.156323e-01</td>\n",
       "      <td>0.802372</td>\n",
       "      <td>0.801287</td>\n",
       "      <td>0.525948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.988100</td>\n",
       "      <td>0.988109</td>\n",
       "      <td>5.082107e-02</td>\n",
       "      <td>0.814229</td>\n",
       "      <td>0.813131</td>\n",
       "      <td>0.572950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.993830</td>\n",
       "      <td>0.993833</td>\n",
       "      <td>2.083978e-02</td>\n",
       "      <td>0.814229</td>\n",
       "      <td>0.811919</td>\n",
       "      <td>0.603155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.998237</td>\n",
       "      <td>0.998237</td>\n",
       "      <td>9.461265e-03</td>\n",
       "      <td>0.814229</td>\n",
       "      <td>0.811569</td>\n",
       "      <td>0.689331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.999559</td>\n",
       "      <td>0.999559</td>\n",
       "      <td>3.848141e-03</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.824204</td>\n",
       "      <td>0.692033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.999559</td>\n",
       "      <td>0.999559</td>\n",
       "      <td>2.033538e-03</td>\n",
       "      <td>0.814229</td>\n",
       "      <td>0.812356</td>\n",
       "      <td>0.803235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.662903e-04</td>\n",
       "      <td>0.814229</td>\n",
       "      <td>0.811512</td>\n",
       "      <td>0.897034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.047035e-04</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.825300</td>\n",
       "      <td>0.945036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.581715e-04</td>\n",
       "      <td>0.802372</td>\n",
       "      <td>0.798774</td>\n",
       "      <td>1.035235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.661091e-05</td>\n",
       "      <td>0.810277</td>\n",
       "      <td>0.807720</td>\n",
       "      <td>1.071421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.790571e-05</td>\n",
       "      <td>0.810277</td>\n",
       "      <td>0.808667</td>\n",
       "      <td>1.095648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.241695e-06</td>\n",
       "      <td>0.810277</td>\n",
       "      <td>0.808211</td>\n",
       "      <td>1.156264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.841164e-06</td>\n",
       "      <td>0.810277</td>\n",
       "      <td>0.807693</td>\n",
       "      <td>1.211300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.033131e-07</td>\n",
       "      <td>0.806324</td>\n",
       "      <td>0.803498</td>\n",
       "      <td>1.253590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.617087e-07</td>\n",
       "      <td>0.806324</td>\n",
       "      <td>0.803487</td>\n",
       "      <td>1.252557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.598761e-08</td>\n",
       "      <td>0.806324</td>\n",
       "      <td>0.803483</td>\n",
       "      <td>1.312015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.264396e-08</td>\n",
       "      <td>0.802372</td>\n",
       "      <td>0.800189</td>\n",
       "      <td>1.309898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.255664e-08</td>\n",
       "      <td>0.802372</td>\n",
       "      <td>0.800348</td>\n",
       "      <td>1.337805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.984038e-09</td>\n",
       "      <td>0.802372</td>\n",
       "      <td>0.799999</td>\n",
       "      <td>1.342091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.672356e-09</td>\n",
       "      <td>0.802372</td>\n",
       "      <td>0.799999</td>\n",
       "      <td>1.353653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.569054e-09</td>\n",
       "      <td>0.802372</td>\n",
       "      <td>0.799999</td>\n",
       "      <td>1.366063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.413212e-09</td>\n",
       "      <td>0.802372</td>\n",
       "      <td>0.799999</td>\n",
       "      <td>1.377516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.360674e-09</td>\n",
       "      <td>0.802372</td>\n",
       "      <td>0.799999</td>\n",
       "      <td>1.381281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.255598e-09</td>\n",
       "      <td>0.802372</td>\n",
       "      <td>0.799999</td>\n",
       "      <td>1.391749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.887830e-09</td>\n",
       "      <td>0.798419</td>\n",
       "      <td>0.796420</td>\n",
       "      <td>1.397931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.835292e-09</td>\n",
       "      <td>0.798419</td>\n",
       "      <td>0.796420</td>\n",
       "      <td>1.400550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.835292e-09</td>\n",
       "      <td>0.798419</td>\n",
       "      <td>0.796420</td>\n",
       "      <td>1.416413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.097983e-09</td>\n",
       "      <td>0.798419</td>\n",
       "      <td>0.796420</td>\n",
       "      <td>1.423573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.308136e-09</td>\n",
       "      <td>0.798419</td>\n",
       "      <td>0.796420</td>\n",
       "      <td>1.437507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.203060e-09</td>\n",
       "      <td>0.794466</td>\n",
       "      <td>0.792472</td>\n",
       "      <td>1.443030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.992906e-09</td>\n",
       "      <td>0.794466</td>\n",
       "      <td>0.792694</td>\n",
       "      <td>1.442035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.308136e-09</td>\n",
       "      <td>0.794466</td>\n",
       "      <td>0.792217</td>\n",
       "      <td>1.443891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.728442e-09</td>\n",
       "      <td>0.794466</td>\n",
       "      <td>0.791975</td>\n",
       "      <td>1.460970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.730215e-09</td>\n",
       "      <td>0.798419</td>\n",
       "      <td>0.795801</td>\n",
       "      <td>1.470649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.728442e-09</td>\n",
       "      <td>0.798419</td>\n",
       "      <td>0.796306</td>\n",
       "      <td>1.473060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.308136e-09</td>\n",
       "      <td>0.798419</td>\n",
       "      <td>0.796306</td>\n",
       "      <td>1.474139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.833518e-09</td>\n",
       "      <td>0.802372</td>\n",
       "      <td>0.799924</td>\n",
       "      <td>1.490040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.570827e-09</td>\n",
       "      <td>0.798419</td>\n",
       "      <td>0.795856</td>\n",
       "      <td>1.484939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.201287e-09</td>\n",
       "      <td>0.802372</td>\n",
       "      <td>0.799924</td>\n",
       "      <td>1.500407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.043672e-09</td>\n",
       "      <td>0.802372</td>\n",
       "      <td>0.799959</td>\n",
       "      <td>1.522503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.358901e-09</td>\n",
       "      <td>0.798419</td>\n",
       "      <td>0.795872</td>\n",
       "      <td>1.526572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.991133e-09</td>\n",
       "      <td>0.802372</td>\n",
       "      <td>0.799467</td>\n",
       "      <td>1.519041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.621593e-09</td>\n",
       "      <td>0.798419</td>\n",
       "      <td>0.795872</td>\n",
       "      <td>1.536750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.779207e-09</td>\n",
       "      <td>0.798419</td>\n",
       "      <td>0.795872</td>\n",
       "      <td>1.539426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.516516e-09</td>\n",
       "      <td>0.802372</td>\n",
       "      <td>0.799467</td>\n",
       "      <td>1.556776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.199513e-09</td>\n",
       "      <td>0.802372</td>\n",
       "      <td>0.799467</td>\n",
       "      <td>1.556440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.884284e-09</td>\n",
       "      <td>0.802372</td>\n",
       "      <td>0.799488</td>\n",
       "      <td>1.573393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  accuracy  f1_score          loss  val_accuracy  val_f1_score  \\\n",
       "0       0  0.561922  0.525717  1.255105e+00      0.774704      0.770956   \n",
       "1       1  0.854121  0.853032  5.430162e-01      0.790514      0.790615   \n",
       "2       2  0.929484  0.929341  2.573461e-01      0.822134      0.819784   \n",
       "3       3  0.976642  0.976646  1.156323e-01      0.802372      0.801287   \n",
       "4       4  0.988100  0.988109  5.082107e-02      0.814229      0.813131   \n",
       "5       5  0.993830  0.993833  2.083978e-02      0.814229      0.811919   \n",
       "6       6  0.998237  0.998237  9.461265e-03      0.814229      0.811569   \n",
       "7       7  0.999559  0.999559  3.848141e-03      0.826087      0.824204   \n",
       "8       8  0.999559  0.999559  2.033538e-03      0.814229      0.812356   \n",
       "9       9  1.000000  1.000000  5.662903e-04      0.814229      0.811512   \n",
       "10     10  1.000000  1.000000  3.047035e-04      0.826087      0.825300   \n",
       "11     11  1.000000  1.000000  1.581715e-04      0.802372      0.798774   \n",
       "12     12  1.000000  1.000000  5.661091e-05      0.810277      0.807720   \n",
       "13     13  1.000000  1.000000  1.790571e-05      0.810277      0.808667   \n",
       "14     14  1.000000  1.000000  8.241695e-06      0.810277      0.808211   \n",
       "15     15  1.000000  1.000000  5.841164e-06      0.810277      0.807693   \n",
       "16     16  1.000000  1.000000  5.033131e-07      0.806324      0.803498   \n",
       "17     17  1.000000  1.000000  1.617087e-07      0.806324      0.803487   \n",
       "18     18  1.000000  1.000000  6.598761e-08      0.806324      0.803483   \n",
       "19     19  1.000000  1.000000  2.264396e-08      0.802372      0.800189   \n",
       "20     20  1.000000  1.000000  1.255664e-08      0.802372      0.800348   \n",
       "21     21  1.000000  1.000000  8.984038e-09      0.802372      0.799999   \n",
       "22     22  1.000000  1.000000  6.672356e-09      0.802372      0.799999   \n",
       "23     23  1.000000  1.000000  5.569054e-09      0.802372      0.799999   \n",
       "24     24  1.000000  1.000000  4.413212e-09      0.802372      0.799999   \n",
       "25     25  1.000000  1.000000  4.360674e-09      0.802372      0.799999   \n",
       "26     26  1.000000  1.000000  4.255598e-09      0.802372      0.799999   \n",
       "27     27  1.000000  1.000000  3.887830e-09      0.798419      0.796420   \n",
       "28     28  1.000000  1.000000  3.835292e-09      0.798419      0.796420   \n",
       "29     29  1.000000  1.000000  3.835292e-09      0.798419      0.796420   \n",
       "30     30  1.000000  1.000000  4.097983e-09      0.798419      0.796420   \n",
       "31     31  1.000000  1.000000  4.308136e-09      0.798419      0.796420   \n",
       "32     32  1.000000  1.000000  4.203060e-09      0.794466      0.792472   \n",
       "33     33  1.000000  1.000000  3.992906e-09      0.794466      0.792694   \n",
       "34     34  1.000000  1.000000  4.308136e-09      0.794466      0.792217   \n",
       "35     35  1.000000  1.000000  4.728442e-09      0.794466      0.791975   \n",
       "36     36  1.000000  1.000000  3.730215e-09      0.798419      0.795801   \n",
       "37     37  1.000000  1.000000  4.728442e-09      0.798419      0.796306   \n",
       "38     38  1.000000  1.000000  4.308136e-09      0.798419      0.796306   \n",
       "39     39  1.000000  1.000000  4.833518e-09      0.802372      0.799924   \n",
       "40     40  1.000000  1.000000  4.570827e-09      0.798419      0.795856   \n",
       "41     41  1.000000  1.000000  5.201287e-09      0.802372      0.799924   \n",
       "42     42  1.000000  1.000000  5.043672e-09      0.802372      0.799959   \n",
       "43     43  1.000000  1.000000  5.358901e-09      0.798419      0.795872   \n",
       "44     44  1.000000  1.000000  4.991133e-09      0.802372      0.799467   \n",
       "45     45  1.000000  1.000000  5.621593e-09      0.798419      0.795872   \n",
       "46     46  1.000000  1.000000  5.779207e-09      0.798419      0.795872   \n",
       "47     47  1.000000  1.000000  5.516516e-09      0.802372      0.799467   \n",
       "48     48  1.000000  1.000000  6.199513e-09      0.802372      0.799467   \n",
       "49     49  1.000000  1.000000  5.884284e-09      0.802372      0.799488   \n",
       "\n",
       "    val_loss  \n",
       "0   0.800958  \n",
       "1   0.593165  \n",
       "2   0.524282  \n",
       "3   0.525948  \n",
       "4   0.572950  \n",
       "5   0.603155  \n",
       "6   0.689331  \n",
       "7   0.692033  \n",
       "8   0.803235  \n",
       "9   0.897034  \n",
       "10  0.945036  \n",
       "11  1.035235  \n",
       "12  1.071421  \n",
       "13  1.095648  \n",
       "14  1.156264  \n",
       "15  1.211300  \n",
       "16  1.253590  \n",
       "17  1.252557  \n",
       "18  1.312015  \n",
       "19  1.309898  \n",
       "20  1.337805  \n",
       "21  1.342091  \n",
       "22  1.353653  \n",
       "23  1.366063  \n",
       "24  1.377516  \n",
       "25  1.381281  \n",
       "26  1.391749  \n",
       "27  1.397931  \n",
       "28  1.400550  \n",
       "29  1.416413  \n",
       "30  1.423573  \n",
       "31  1.437507  \n",
       "32  1.443030  \n",
       "33  1.442035  \n",
       "34  1.443891  \n",
       "35  1.460970  \n",
       "36  1.470649  \n",
       "37  1.473060  \n",
       "38  1.474139  \n",
       "39  1.490040  \n",
       "40  1.484939  \n",
       "41  1.500407  \n",
       "42  1.522503  \n",
       "43  1.526572  \n",
       "44  1.519041  \n",
       "45  1.536750  \n",
       "46  1.539426  \n",
       "47  1.556776  \n",
       "48  1.556440  \n",
       "49  1.573393  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_data = pd.read_csv('log/W2V/training.log', sep = ',', engine = 'python')\n",
    "log_data.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f716de4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    50.000000\n",
       "mean      0.803399\n",
       "std       0.008813\n",
       "min       0.774704\n",
       "25%       0.798419\n",
       "50%       0.802372\n",
       "75%       0.806324\n",
       "max       0.826087\n",
       "Name: val_accuracy, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_data['val_accuracy'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "240bd05e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    50.000000\n",
       "mean      0.801115\n",
       "std       0.008961\n",
       "min       0.770956\n",
       "25%       0.796335\n",
       "50%       0.799942\n",
       "75%       0.803495\n",
       "max       0.825300\n",
       "Name: val_f1_score, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_data['val_f1_score'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60a76194",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 5ms/step\n",
      "Accuracy: 0.8023715415019763\n",
      "F1 score: 0.7994878277014907\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.83      0.76        35\n",
      "           1       0.83      0.88      0.85        96\n",
      "           2       0.82      0.56      0.67        32\n",
      "           3       0.84      0.90      0.87        30\n",
      "           4       0.71      0.71      0.71        28\n",
      "           5       0.86      0.78      0.82        32\n",
      "\n",
      "    accuracy                           0.80       253\n",
      "   macro avg       0.80      0.78      0.78       253\n",
      "weighted avg       0.81      0.80      0.80       253\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(\"saved_models/W2V/EQCM.h5\")\n",
    "evaluate_model(model, padded_X_test_seq, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8af0283",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data[['val_accuracy']].idxmax() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084d78a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_result(log_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
