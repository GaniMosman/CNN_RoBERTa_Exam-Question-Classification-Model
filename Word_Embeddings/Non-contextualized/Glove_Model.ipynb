{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d0f0e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "seed = 1\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9bec4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "import gensim.downloader as api\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adbdb43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.layers import Dense, Embedding, Conv1D, GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c7a1d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9eab47ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_sequence(questions):\n",
    "    X = questions['Preprocessed_Question']\n",
    "    cognitive_level = {\"Knowledge\": 0, \"Comprehension\": 1, \"Application\": 2, \"Analysis\": 3, \"Synthesis\": 4, \"Evaluation\": 5}\n",
    "    questions[\"BT LEVEL\"].replace(cognitive_level, inplace = True)\n",
    "    y = questions['BT LEVEL'].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y , test_size = 0.10, \n",
    "                                        stratify = questions['BT LEVEL'], random_state = 1)\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    \n",
    "    tokenizer = Tokenizer(oov_token ='<OOV>')\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    vocab = tokenizer.word_index\n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "    X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "    max_que_len = max(len(x) for x in X_train_seq)\n",
    "    padded_X_train_seq = pad_sequences(X_train_seq, maxlen = max_que_len, padding = 'post')\n",
    "    padded_X_test_seq = pad_sequences(X_test_seq, maxlen = max_que_len, padding = 'post')\n",
    "    \n",
    "    return padded_X_train_seq, y_train, padded_X_test_seq, y_test, vocab, max_que_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8693fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(vocab):\n",
    "    embedding_model = api.load('glove-wiki-gigaword-300')\n",
    "    embeddings = np.zeros((len(vocab) + 1, 300))\n",
    "    for word, index in vocab.items():\n",
    "        if word in embedding_model.key_to_index:\n",
    "            embedding_vector = embedding_model[word]\n",
    "            embeddings[index] = embedding_vector\n",
    "        else:\n",
    "            print(word)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a30c351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_len, max_que_len, embeddings):\n",
    "    # defining the model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim = vocab_size, output_dim = embedding_len, weights = [embeddings], \n",
    "                    input_length = max_que_len,  trainable = True))\n",
    "    model.add(Conv1D(32, 5, activation ='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(6, activation ='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db4008fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    pred = model.predict(X_test)\n",
    "    predicted_classes = np.argmax(pred, axis = 1)\n",
    "    true_classes = np.argmax(y_test, axis = 1)\n",
    "    accuracy = accuracy_score(true_classes, predicted_classes)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    f1_ = f1_score(true_classes, predicted_classes, average = 'weighted')\n",
    "    print(f\"F1 score: {f1_}\")\n",
    "    print(classification_report(true_classes, predicted_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc3ecc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(log_data): \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(log_data['loss'], label ='train')\n",
    "    plt.plot(log_data['val_loss'], label ='test')\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(log_data['accuracy'], label ='train')\n",
    "    plt.plot(log_data['val_accuracy'], label ='test')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(log_data['f1_score'], label ='train')\n",
    "    plt.plot(log_data['val_f1_score'], label ='test')\n",
    "    plt.title('Model F1 Score')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3af42cbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Preprocessed_Question</th>\n",
       "      <th>BT LEVEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Suppose prices of two goods are constant, expl...</td>\n",
       "      <td>suppose price good constant explain what happe...</td>\n",
       "      <td>Comprehension</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Explain the concept of price leadership observ...</td>\n",
       "      <td>explain concept price leadership observe condi...</td>\n",
       "      <td>Comprehension</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Define profit. Briefly explain how accounting ...</td>\n",
       "      <td>define profit briefly explain how account prof...</td>\n",
       "      <td>Comprehension</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Describe the assumptions of monopolistic compe...</td>\n",
       "      <td>describe assumption monopolistic competitive m...</td>\n",
       "      <td>Comprehension</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Explain the meaning of the law of diminishing ...</td>\n",
       "      <td>explain mean law diminish marginal return brie...</td>\n",
       "      <td>Comprehension</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2517</th>\n",
       "      <td>PEST and SWOT are popular strategy tools. Disc...</td>\n",
       "      <td>pest swot popular strategy tool discuss how to...</td>\n",
       "      <td>Application</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2518</th>\n",
       "      <td>List the advantages and disadvantages of Publi...</td>\n",
       "      <td>list advantage disadvantage public offer</td>\n",
       "      <td>Knowledge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2519</th>\n",
       "      <td>Show your calculations for all THREE (3) optio...</td>\n",
       "      <td>show calculation option discuss which option p...</td>\n",
       "      <td>Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2520</th>\n",
       "      <td>Currently the product life cycle for Apple iPo...</td>\n",
       "      <td>currently product life cycle apple ipod growth...</td>\n",
       "      <td>Evaluation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2521</th>\n",
       "      <td>Define brand audit.</td>\n",
       "      <td>define brand audit</td>\n",
       "      <td>Knowledge</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2522 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Question  \\\n",
       "0     Suppose prices of two goods are constant, expl...   \n",
       "1     Explain the concept of price leadership observ...   \n",
       "2     Define profit. Briefly explain how accounting ...   \n",
       "3     Describe the assumptions of monopolistic compe...   \n",
       "4     Explain the meaning of the law of diminishing ...   \n",
       "...                                                 ...   \n",
       "2517  PEST and SWOT are popular strategy tools. Disc...   \n",
       "2518  List the advantages and disadvantages of Publi...   \n",
       "2519  Show your calculations for all THREE (3) optio...   \n",
       "2520  Currently the product life cycle for Apple iPo...   \n",
       "2521                                Define brand audit.   \n",
       "\n",
       "                                  Preprocessed_Question       BT LEVEL  \n",
       "0     suppose price good constant explain what happe...  Comprehension  \n",
       "1     explain concept price leadership observe condi...  Comprehension  \n",
       "2     define profit briefly explain how account prof...  Comprehension  \n",
       "3     describe assumption monopolistic competitive m...  Comprehension  \n",
       "4     explain mean law diminish marginal return brie...  Comprehension  \n",
       "...                                                 ...            ...  \n",
       "2517  pest swot popular strategy tool discuss how to...    Application  \n",
       "2518           list advantage disadvantage public offer      Knowledge  \n",
       "2519  show calculation option discuss which option p...       Analysis  \n",
       "2520  currently product life cycle apple ipod growth...     Evaluation  \n",
       "2521                                 define brand audit      Knowledge  \n",
       "\n",
       "[2522 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = pd.read_excel('preprocessing_result/preprocessing_result-glove-wiki.xlsx')\n",
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "260da8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_X_train_seq, y_train, padded_X_test_seq, y_test, vocab, max_que_len = convert_to_sequence(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00df7e54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<OOV>\n",
      "aov\n",
      "infomediary\n",
      "phosphoru\n",
      "mccg\n",
      "komugi\n",
      "eoq\n",
      "elute\n",
      "finfet\n",
      "utar\n",
      "sqrt\n",
      "chemostat\n",
      "radiobutton\n",
      "adts\n",
      "kaseem\n",
      "perakian\n",
      "junebank\n",
      "firedup\n",
      "qlassic\n",
      "mirr\n",
      "technopreneur\n",
      "pondy\n",
      "chemisorption\n",
      "quasineutral\n",
      "openerp\n",
      "spiration\n",
      "electroosmosis\n",
      "hexapeptide\n",
      "kmk\n",
      "nextseq\n",
      "passivator\n",
      "arraylist\n",
      "bubblesort\n",
      "matrik\n",
      "putable\n",
      "putlog\n",
      "blosum\n",
      "monopolistically\n",
      "dictogloss\n",
      "hyperarid\n",
      "eqrnhl\n",
      "neolocal\n",
      "sethour\n",
      "sendredirect\n",
      "linebreeding\n",
      "phenylpropanoid\n",
      "pourbaix\n",
      "minisatellite\n",
      "oxyperoxidase\n",
      "luedeking\n",
      "pullulan\n",
      "struc\n",
      "mesofauna\n",
      "cominunicate\n",
      "medoids\n",
      "diskretionary\n",
      "langmiur\n",
      "isoterm\n",
      "requestdispatcher\n",
      "mircroscope\n",
      "higg\n",
      "httpsessionactivationlistener\n",
      "mincut\n",
      "determinental\n",
      "schubli\n",
      "mediuin\n",
      "sqykrecp\n",
      "myner\n",
      "programrs\n",
      "wmeadk\n",
      "dmvrel\n",
      "mmlr\n",
      "allelopathy\n",
      "luzonensis\n",
      "ionizable\n",
      "prenyltransferase\n",
      "exarnple\n",
      "blastx\n",
      "pnk\n",
      "equillibrium\n",
      "musictype\n",
      "mytune\n",
      "fupo\n",
      "placentome\n",
      "transabdominal\n",
      "matsuhara\n",
      "deae\n",
      "sephadex\n",
      "jstl\n",
      "outbred\n",
      "elaeis\n",
      "eisenthal\n",
      "modelessform\n",
      "showdialog\n",
      "herbatious\n",
      "jfet\n",
      "makamat\n",
      "incfromn\n",
      "sooosay\n",
      "outli\n",
      "ytm\n",
      "komive\n",
      "tblastx\n",
      "viestal\n",
      "atcase\n",
      "communalitie\n",
      "miscroscope\n"
     ]
    }
   ],
   "source": [
    "# embeddings_glove_wiki = get_embedding(vocab)\n",
    "# np.save('embedding/glove/embeddings.npy', embeddings_glove_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06f56e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_glove_wiki = np.load('embedding/glove/embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b48c3a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#callbacks \n",
    "training_logger = CSVLogger('log/Glove/training_wiki.log', separator = ',', append = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cd1ba60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 44, 300)           1114200   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 40, 32)            48032     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 32)               0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 6)                 198       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,162,430\n",
      "Trainable params: 1,162,430\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model\n",
    "model = build_model(len(vocab) + 1, 300, max_que_len, embeddings_glove_wiki)\n",
    "# compiling the model\n",
    "model.compile(optimizer = 'RMSprop', loss='categorical_crossentropy', metrics=['accuracy', tfa.metrics.F1Score(6, 'weighted')])\n",
    "# printing summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e3f4562",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "284/284 [==============================] - 4s 10ms/step - loss: 1.2790 - accuracy: 0.5381 - f1_score: 0.5095 - val_loss: 0.8616 - val_accuracy: 0.7391 - val_f1_score: 0.7321\n",
      "Epoch 2/50\n",
      "284/284 [==============================] - 3s 9ms/step - loss: 0.5306 - accuracy: 0.8475 - f1_score: 0.8463 - val_loss: 0.7353 - val_accuracy: 0.7787 - val_f1_score: 0.7748\n",
      "Epoch 3/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 0.2300 - accuracy: 0.9493 - f1_score: 0.9493 - val_loss: 0.6798 - val_accuracy: 0.7905 - val_f1_score: 0.7855\n",
      "Epoch 4/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 0.0873 - accuracy: 0.9872 - f1_score: 0.9872 - val_loss: 0.6756 - val_accuracy: 0.7628 - val_f1_score: 0.7629\n",
      "Epoch 5/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 0.0354 - accuracy: 0.9907 - f1_score: 0.9907 - val_loss: 0.7785 - val_accuracy: 0.7708 - val_f1_score: 0.7675\n",
      "Epoch 6/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 0.0126 - accuracy: 0.9982 - f1_score: 0.9982 - val_loss: 0.8395 - val_accuracy: 0.7787 - val_f1_score: 0.7749\n",
      "Epoch 7/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 0.0046 - accuracy: 0.9991 - f1_score: 0.9991 - val_loss: 1.0007 - val_accuracy: 0.7668 - val_f1_score: 0.7633\n",
      "Epoch 8/50\n",
      "284/284 [==============================] - 2s 9ms/step - loss: 0.0024 - accuracy: 0.9991 - f1_score: 0.9991 - val_loss: 1.0144 - val_accuracy: 0.7945 - val_f1_score: 0.7927\n",
      "Epoch 9/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 6.0347e-04 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.0371 - val_accuracy: 0.7905 - val_f1_score: 0.7886\n",
      "Epoch 10/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 2.2301e-04 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.1959 - val_accuracy: 0.7747 - val_f1_score: 0.7698\n",
      "Epoch 11/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 2.4078e-04 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.2571 - val_accuracy: 0.7708 - val_f1_score: 0.7672\n",
      "Epoch 12/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 5.4022e-05 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.4068 - val_accuracy: 0.7787 - val_f1_score: 0.7761\n",
      "Epoch 13/50\n",
      "284/284 [==============================] - 3s 9ms/step - loss: 8.2093e-05 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.4356 - val_accuracy: 0.7747 - val_f1_score: 0.7714\n",
      "Epoch 14/50\n",
      "284/284 [==============================] - 3s 9ms/step - loss: 2.8378e-06 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.4877 - val_accuracy: 0.7747 - val_f1_score: 0.7708\n",
      "Epoch 15/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 2.3842e-06 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.5296 - val_accuracy: 0.7826 - val_f1_score: 0.7798\n",
      "Epoch 16/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 6.2087e-07 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.5708 - val_accuracy: 0.7787 - val_f1_score: 0.7751\n",
      "Epoch 17/50\n",
      "284/284 [==============================] - 2s 9ms/step - loss: 1.5193e-07 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.6109 - val_accuracy: 0.7826 - val_f1_score: 0.7791\n",
      "Epoch 18/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 3.1785e-08 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.6370 - val_accuracy: 0.7826 - val_f1_score: 0.7791\n",
      "Epoch 19/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 1.2557e-08 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.6752 - val_accuracy: 0.7866 - val_f1_score: 0.7827\n",
      "Epoch 20/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 7.5655e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.6653 - val_accuracy: 0.7826 - val_f1_score: 0.7791\n",
      "Epoch 21/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 5.9368e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.6948 - val_accuracy: 0.7826 - val_f1_score: 0.7791\n",
      "Epoch 22/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 5.4640e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.7142 - val_accuracy: 0.7866 - val_f1_score: 0.7827\n",
      "Epoch 23/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 5.2538e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.7201 - val_accuracy: 0.7826 - val_f1_score: 0.7791\n",
      "Epoch 24/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 4.6234e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.7360 - val_accuracy: 0.7866 - val_f1_score: 0.7827\n",
      "Epoch 25/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 4.6234e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.7747 - val_accuracy: 0.7787 - val_f1_score: 0.7752\n",
      "Epoch 26/50\n",
      "284/284 [==============================] - 2s 9ms/step - loss: 4.5708e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.7655 - val_accuracy: 0.7866 - val_f1_score: 0.7830\n",
      "Epoch 27/50\n",
      "284/284 [==============================] - 3s 9ms/step - loss: 5.2538e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.7862 - val_accuracy: 0.7826 - val_f1_score: 0.7793\n",
      "Epoch 28/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 5.3064e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.7834 - val_accuracy: 0.7826 - val_f1_score: 0.7792\n",
      "Epoch 29/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 4.9911e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.7940 - val_accuracy: 0.7826 - val_f1_score: 0.7792\n",
      "Epoch 30/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 4.8335e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.8336 - val_accuracy: 0.7826 - val_f1_score: 0.7791\n",
      "Epoch 31/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 4.7810e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.8230 - val_accuracy: 0.7826 - val_f1_score: 0.7791\n",
      "Epoch 32/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 5.6216e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.8636 - val_accuracy: 0.7826 - val_f1_score: 0.7791\n",
      "Epoch 33/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 4.9911e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.8498 - val_accuracy: 0.7826 - val_f1_score: 0.7801\n",
      "Epoch 34/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 5.7267e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.8775 - val_accuracy: 0.7866 - val_f1_score: 0.7841\n",
      "Epoch 35/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 6.1995e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.8898 - val_accuracy: 0.7866 - val_f1_score: 0.7855\n",
      "Epoch 36/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 6.4622e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.9229 - val_accuracy: 0.7866 - val_f1_score: 0.7844\n",
      "Epoch 37/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 6.1470e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.9353 - val_accuracy: 0.7905 - val_f1_score: 0.7893\n",
      "Epoch 38/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 6.7774e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.9846 - val_accuracy: 0.7826 - val_f1_score: 0.7802\n",
      "Epoch 39/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 7.4079e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 1.9886 - val_accuracy: 0.7905 - val_f1_score: 0.7891\n",
      "Epoch 40/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 8.1434e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 2.0177 - val_accuracy: 0.7787 - val_f1_score: 0.7776\n",
      "Epoch 41/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 7.4079e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 2.0245 - val_accuracy: 0.7866 - val_f1_score: 0.7857\n",
      "Epoch 42/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 8.0909e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 2.0258 - val_accuracy: 0.7826 - val_f1_score: 0.7818\n",
      "Epoch 43/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 8.0909e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 2.0747 - val_accuracy: 0.7826 - val_f1_score: 0.7821\n",
      "Epoch 44/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 8.9840e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 2.0957 - val_accuracy: 0.7866 - val_f1_score: 0.7857\n",
      "Epoch 45/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 9.4569e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 2.0952 - val_accuracy: 0.7787 - val_f1_score: 0.7785\n",
      "Epoch 46/50\n",
      "284/284 [==============================] - 2s 9ms/step - loss: 8.9315e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 2.1226 - val_accuracy: 0.7826 - val_f1_score: 0.7824\n",
      "Epoch 47/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 9.8247e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 2.2039 - val_accuracy: 0.7826 - val_f1_score: 0.7823\n",
      "Epoch 48/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 1.0823e-08 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 2.1856 - val_accuracy: 0.7866 - val_f1_score: 0.7857\n",
      "Epoch 49/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 8.8790e-09 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 2.2008 - val_accuracy: 0.7787 - val_f1_score: 0.7783\n",
      "Epoch 50/50\n",
      "284/284 [==============================] - 2s 8ms/step - loss: 1.2452e-08 - accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 2.2203 - val_accuracy: 0.7787 - val_f1_score: 0.7783\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(padded_X_train_seq, y_train, epochs = 50, batch_size = 8, validation_data = (padded_X_test_seq, y_test),\n",
    "                                                                      callbacks = [training_logger], verbose = 1)\n",
    "#saving the trained model\n",
    "model.save('saved_models/Glove/EQCM_wiki.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd9d157f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_f1_score</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.538123</td>\n",
       "      <td>0.509456</td>\n",
       "      <td>1.278968e+00</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.732118</td>\n",
       "      <td>0.861599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.847510</td>\n",
       "      <td>0.846347</td>\n",
       "      <td>5.305675e-01</td>\n",
       "      <td>0.778656</td>\n",
       "      <td>0.774834</td>\n",
       "      <td>0.735253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.949317</td>\n",
       "      <td>0.949271</td>\n",
       "      <td>2.299964e-01</td>\n",
       "      <td>0.790514</td>\n",
       "      <td>0.785471</td>\n",
       "      <td>0.679846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.987219</td>\n",
       "      <td>0.987248</td>\n",
       "      <td>8.729480e-02</td>\n",
       "      <td>0.762846</td>\n",
       "      <td>0.762895</td>\n",
       "      <td>0.675591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.990745</td>\n",
       "      <td>0.990739</td>\n",
       "      <td>3.536326e-02</td>\n",
       "      <td>0.770751</td>\n",
       "      <td>0.767454</td>\n",
       "      <td>0.778476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.998237</td>\n",
       "      <td>0.998237</td>\n",
       "      <td>1.262203e-02</td>\n",
       "      <td>0.778656</td>\n",
       "      <td>0.774937</td>\n",
       "      <td>0.839458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.999119</td>\n",
       "      <td>0.999118</td>\n",
       "      <td>4.558954e-03</td>\n",
       "      <td>0.766798</td>\n",
       "      <td>0.763349</td>\n",
       "      <td>1.000701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.999119</td>\n",
       "      <td>0.999119</td>\n",
       "      <td>2.404380e-03</td>\n",
       "      <td>0.794466</td>\n",
       "      <td>0.792718</td>\n",
       "      <td>1.014408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.034739e-04</td>\n",
       "      <td>0.790514</td>\n",
       "      <td>0.788600</td>\n",
       "      <td>1.037116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.230115e-04</td>\n",
       "      <td>0.774704</td>\n",
       "      <td>0.769833</td>\n",
       "      <td>1.195944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.407804e-04</td>\n",
       "      <td>0.770751</td>\n",
       "      <td>0.767184</td>\n",
       "      <td>1.257105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.402196e-05</td>\n",
       "      <td>0.778656</td>\n",
       "      <td>0.776109</td>\n",
       "      <td>1.406787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.209261e-05</td>\n",
       "      <td>0.774704</td>\n",
       "      <td>0.771377</td>\n",
       "      <td>1.435627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.837754e-06</td>\n",
       "      <td>0.774704</td>\n",
       "      <td>0.770795</td>\n",
       "      <td>1.487739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.384248e-06</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.779830</td>\n",
       "      <td>1.529607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.208732e-07</td>\n",
       "      <td>0.778656</td>\n",
       "      <td>0.775088</td>\n",
       "      <td>1.570837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.519349e-07</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.779140</td>\n",
       "      <td>1.610948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.178546e-08</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.779140</td>\n",
       "      <td>1.637032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.255663e-08</td>\n",
       "      <td>0.786561</td>\n",
       "      <td>0.782733</td>\n",
       "      <td>1.675218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.565506e-09</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.779140</td>\n",
       "      <td>1.665287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.936821e-09</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.779140</td>\n",
       "      <td>1.694820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.463977e-09</td>\n",
       "      <td>0.786561</td>\n",
       "      <td>0.782733</td>\n",
       "      <td>1.714172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.253824e-09</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.779140</td>\n",
       "      <td>1.720149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.623366e-09</td>\n",
       "      <td>0.786561</td>\n",
       "      <td>0.782733</td>\n",
       "      <td>1.736008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.623366e-09</td>\n",
       "      <td>0.778656</td>\n",
       "      <td>0.775179</td>\n",
       "      <td>1.774750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.570827e-09</td>\n",
       "      <td>0.786561</td>\n",
       "      <td>0.783011</td>\n",
       "      <td>1.765509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.253824e-09</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.779271</td>\n",
       "      <td>1.786247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.306363e-09</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.779218</td>\n",
       "      <td>1.783449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.991133e-09</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.779218</td>\n",
       "      <td>1.793983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.833518e-09</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.779090</td>\n",
       "      <td>1.833610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.780981e-09</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.779116</td>\n",
       "      <td>1.823048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.621593e-09</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.779116</td>\n",
       "      <td>1.863636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.991133e-09</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.780143</td>\n",
       "      <td>1.849826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.726669e-09</td>\n",
       "      <td>0.786561</td>\n",
       "      <td>0.784070</td>\n",
       "      <td>1.877542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.199513e-09</td>\n",
       "      <td>0.786561</td>\n",
       "      <td>0.785497</td>\n",
       "      <td>1.889801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.462205e-09</td>\n",
       "      <td>0.786561</td>\n",
       "      <td>0.784395</td>\n",
       "      <td>1.922910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.146975e-09</td>\n",
       "      <td>0.790514</td>\n",
       "      <td>0.789311</td>\n",
       "      <td>1.935317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.777433e-09</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.780242</td>\n",
       "      <td>1.984636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.407893e-09</td>\n",
       "      <td>0.790514</td>\n",
       "      <td>0.789077</td>\n",
       "      <td>1.988603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.143427e-09</td>\n",
       "      <td>0.778656</td>\n",
       "      <td>0.777641</td>\n",
       "      <td>2.017682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.407893e-09</td>\n",
       "      <td>0.786561</td>\n",
       "      <td>0.785738</td>\n",
       "      <td>2.024492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.090890e-09</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.781824</td>\n",
       "      <td>2.025846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.090890e-09</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.782105</td>\n",
       "      <td>2.074705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.984039e-09</td>\n",
       "      <td>0.786561</td>\n",
       "      <td>0.785738</td>\n",
       "      <td>2.095675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.456884e-09</td>\n",
       "      <td>0.778656</td>\n",
       "      <td>0.778500</td>\n",
       "      <td>2.095200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.931502e-09</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.782411</td>\n",
       "      <td>2.122606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.824651e-09</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.782339</td>\n",
       "      <td>2.203921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.082288e-08</td>\n",
       "      <td>0.786561</td>\n",
       "      <td>0.785707</td>\n",
       "      <td>2.185559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.878963e-09</td>\n",
       "      <td>0.778656</td>\n",
       "      <td>0.778327</td>\n",
       "      <td>2.200781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.245156e-08</td>\n",
       "      <td>0.778656</td>\n",
       "      <td>0.778327</td>\n",
       "      <td>2.220296</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  accuracy  f1_score          loss  val_accuracy  val_f1_score  \\\n",
       "0       0  0.538123  0.509456  1.278968e+00      0.739130      0.732118   \n",
       "1       1  0.847510  0.846347  5.305675e-01      0.778656      0.774834   \n",
       "2       2  0.949317  0.949271  2.299964e-01      0.790514      0.785471   \n",
       "3       3  0.987219  0.987248  8.729480e-02      0.762846      0.762895   \n",
       "4       4  0.990745  0.990739  3.536326e-02      0.770751      0.767454   \n",
       "5       5  0.998237  0.998237  1.262203e-02      0.778656      0.774937   \n",
       "6       6  0.999119  0.999118  4.558954e-03      0.766798      0.763349   \n",
       "7       7  0.999119  0.999119  2.404380e-03      0.794466      0.792718   \n",
       "8       8  1.000000  1.000000  6.034739e-04      0.790514      0.788600   \n",
       "9       9  1.000000  1.000000  2.230115e-04      0.774704      0.769833   \n",
       "10     10  1.000000  1.000000  2.407804e-04      0.770751      0.767184   \n",
       "11     11  1.000000  1.000000  5.402196e-05      0.778656      0.776109   \n",
       "12     12  1.000000  1.000000  8.209261e-05      0.774704      0.771377   \n",
       "13     13  1.000000  1.000000  2.837754e-06      0.774704      0.770795   \n",
       "14     14  1.000000  1.000000  2.384248e-06      0.782609      0.779830   \n",
       "15     15  1.000000  1.000000  6.208732e-07      0.778656      0.775088   \n",
       "16     16  1.000000  1.000000  1.519349e-07      0.782609      0.779140   \n",
       "17     17  1.000000  1.000000  3.178546e-08      0.782609      0.779140   \n",
       "18     18  1.000000  1.000000  1.255663e-08      0.786561      0.782733   \n",
       "19     19  1.000000  1.000000  7.565506e-09      0.782609      0.779140   \n",
       "20     20  1.000000  1.000000  5.936821e-09      0.782609      0.779140   \n",
       "21     21  1.000000  1.000000  5.463977e-09      0.786561      0.782733   \n",
       "22     22  1.000000  1.000000  5.253824e-09      0.782609      0.779140   \n",
       "23     23  1.000000  1.000000  4.623366e-09      0.786561      0.782733   \n",
       "24     24  1.000000  1.000000  4.623366e-09      0.778656      0.775179   \n",
       "25     25  1.000000  1.000000  4.570827e-09      0.786561      0.783011   \n",
       "26     26  1.000000  1.000000  5.253824e-09      0.782609      0.779271   \n",
       "27     27  1.000000  1.000000  5.306363e-09      0.782609      0.779218   \n",
       "28     28  1.000000  1.000000  4.991133e-09      0.782609      0.779218   \n",
       "29     29  1.000000  1.000000  4.833518e-09      0.782609      0.779090   \n",
       "30     30  1.000000  1.000000  4.780981e-09      0.782609      0.779116   \n",
       "31     31  1.000000  1.000000  5.621593e-09      0.782609      0.779116   \n",
       "32     32  1.000000  1.000000  4.991133e-09      0.782609      0.780143   \n",
       "33     33  1.000000  1.000000  5.726669e-09      0.786561      0.784070   \n",
       "34     34  1.000000  1.000000  6.199513e-09      0.786561      0.785497   \n",
       "35     35  1.000000  1.000000  6.462205e-09      0.786561      0.784395   \n",
       "36     36  1.000000  1.000000  6.146975e-09      0.790514      0.789311   \n",
       "37     37  1.000000  1.000000  6.777433e-09      0.782609      0.780242   \n",
       "38     38  1.000000  1.000000  7.407893e-09      0.790514      0.789077   \n",
       "39     39  1.000000  1.000000  8.143427e-09      0.778656      0.777641   \n",
       "40     40  1.000000  1.000000  7.407893e-09      0.786561      0.785738   \n",
       "41     41  1.000000  1.000000  8.090890e-09      0.782609      0.781824   \n",
       "42     42  1.000000  1.000000  8.090890e-09      0.782609      0.782105   \n",
       "43     43  1.000000  1.000000  8.984039e-09      0.786561      0.785738   \n",
       "44     44  1.000000  1.000000  9.456884e-09      0.778656      0.778500   \n",
       "45     45  1.000000  1.000000  8.931502e-09      0.782609      0.782411   \n",
       "46     46  1.000000  1.000000  9.824651e-09      0.782609      0.782339   \n",
       "47     47  1.000000  1.000000  1.082288e-08      0.786561      0.785707   \n",
       "48     48  1.000000  1.000000  8.878963e-09      0.778656      0.778327   \n",
       "49     49  1.000000  1.000000  1.245156e-08      0.778656      0.778327   \n",
       "\n",
       "    val_loss  \n",
       "0   0.861599  \n",
       "1   0.735253  \n",
       "2   0.679846  \n",
       "3   0.675591  \n",
       "4   0.778476  \n",
       "5   0.839458  \n",
       "6   1.000701  \n",
       "7   1.014408  \n",
       "8   1.037116  \n",
       "9   1.195944  \n",
       "10  1.257105  \n",
       "11  1.406787  \n",
       "12  1.435627  \n",
       "13  1.487739  \n",
       "14  1.529607  \n",
       "15  1.570837  \n",
       "16  1.610948  \n",
       "17  1.637032  \n",
       "18  1.675218  \n",
       "19  1.665287  \n",
       "20  1.694820  \n",
       "21  1.714172  \n",
       "22  1.720149  \n",
       "23  1.736008  \n",
       "24  1.774750  \n",
       "25  1.765509  \n",
       "26  1.786247  \n",
       "27  1.783449  \n",
       "28  1.793983  \n",
       "29  1.833610  \n",
       "30  1.823048  \n",
       "31  1.863636  \n",
       "32  1.849826  \n",
       "33  1.877542  \n",
       "34  1.889801  \n",
       "35  1.922910  \n",
       "36  1.935317  \n",
       "37  1.984636  \n",
       "38  1.988603  \n",
       "39  2.017682  \n",
       "40  2.024492  \n",
       "41  2.025846  \n",
       "42  2.074705  \n",
       "43  2.095675  \n",
       "44  2.095200  \n",
       "45  2.122606  \n",
       "46  2.203921  \n",
       "47  2.185559  \n",
       "48  2.200781  \n",
       "49  2.220296  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_data = pd.read_csv('log/Glove/training_wiki.log', sep = ',', engine = 'python')\n",
    "log_data.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f716de4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    50.000000\n",
       "mean      0.781028\n",
       "std       0.008526\n",
       "min       0.739130\n",
       "25%       0.778656\n",
       "50%       0.782609\n",
       "75%       0.786561\n",
       "max       0.794466\n",
       "Name: val_accuracy, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_data['val_accuracy'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bee2538f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    50.000000\n",
       "mean      0.778422\n",
       "std       0.009156\n",
       "min       0.732118\n",
       "25%       0.776492\n",
       "50%       0.779179\n",
       "75%       0.782733\n",
       "max       0.792718\n",
       "Name: val_f1_score, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_data['val_f1_score'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3a31911",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 5ms/step\n",
      "Accuracy: 0.7786561264822134\n",
      "F1 score: 0.7783267267072054\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.77      0.73        35\n",
      "           1       0.83      0.85      0.84        96\n",
      "           2       0.67      0.62      0.65        32\n",
      "           3       0.86      0.80      0.83        30\n",
      "           4       0.72      0.75      0.74        28\n",
      "           5       0.82      0.72      0.77        32\n",
      "\n",
      "    accuracy                           0.78       253\n",
      "   macro avg       0.76      0.75      0.76       253\n",
      "weighted avg       0.78      0.78      0.78       253\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(\"saved_models/Glove/EQCM_wiki.h5\")\n",
    "evaluate_model(model, padded_X_test_seq, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeb3358",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data[['val_accuracy']].idxmax() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084d78a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_result(log_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
